---
title: "Predicting Bank Customer Attrition"
author: "Geetesh Lokhande"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, out.width="60%", out.height="60%", fig.align='center')
```

```{r results='hide'}
if(!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, dplyr, e1071, rpart, esquisse, ggplot2, ggpubr, reshape, gains, caret, ROCR,pROC, rattle, rpart.plot, partykit, randomForest, vip, xgboost, DMwR, neuralnet,class)
theme_set(theme_minimal())
```

```{r}
##Load the original dataset
raw.df <- read.csv("Churn_Modelling.csv")

##Viewing the dataset
#View(raw.df)
#str(raw.df)

##Removing the columns "RowNumber", "CustomerId","Surname"
bank.df <- raw.df[,c("CreditScore","Geography","Gender","Age","Tenure","Balance",
                     "NumOfProducts","HasCrCard","IsActiveMember","EstimatedSalary","Exited")]
#str(bank.df)



#converting to categorical variables

bank.df$Exited <- as.factor(bank.df$Exited)
bank.raw.df <- bank.df
bank.df$IsActiveMember <- as.factor(bank.df$IsActiveMember)
bank.df$HasCrCard <- as.factor(bank.df$HasCrCard)
bank.df$NumOfProducts <- as.factor(bank.df$NumOfProducts)
bank.df$Tenure <- as.factor(bank.df$Tenure)
#glimpse(bank.df)

##checking for any missing values
#which(is.na(bank.df))
#sapply(bank.df, function(x) sum(is.na(x)))
##There is not any missing data


```

```{r}
## Checking the overview of the dataset
#summary(bank.df)

#esquisse::esquisser()

## Bar Chart showing the dsitribution of the Response Variable
response.plot <- ggplot(bank.df) +
 aes(x = Exited, fill = Exited) +
 geom_bar() +
 scale_fill_brewer(palette = "Accent") +
 labs(x = "Churned", title = "Bar Chart showing the dsitribution of the Response Variable \"Exited\"") +
 theme_minimal() +
 theme(legend.position = "left") + 
  theme_minimal() + geom_text(aes(label = ..count..), stat = "count", position = position_dodge(width = 0.8))


## Distribution of numeric features
numeric.plot <- bank.df %>%
  keep(is.numeric) %>%
  gather() %>%
  ggplot() +
  geom_histogram(mapping = aes(x=value,fill=key), color="black") +
  labs(title = "Histograms showing the distribution of Numerical features \"Exited\"") +
 scale_fill_brewer(palette = "Accent") +
  facet_wrap(~ key, scales = "free") +
  theme_minimal() +
  theme(legend.position = 'none')


## Side-by-side bar graph showing gender distribution in customer churn
gender.plot <- ggplot(bank.df) +
 aes(x = Gender, fill = Exited) +
 geom_bar(position = "dodge") +
 scale_fill_brewer(palette = "Accent") +
 labs(y = "count", title = "Side-by-side bar graph showing gender distribution in customer churn") +
 theme_minimal() + geom_text(aes(label = ..count..), stat = "count", position = position_dodge(width = 0.8))


## Side-by-side bar graph showing the number of products a customers owns and the churn
numOfProducts.plot <- ggplot(bank.df) +
 aes(x = NumOfProducts, fill = Exited) +
 geom_bar(position = "dodge") +
 scale_fill_brewer(palette = "Accent") +
 labs(x = "Number of Products", title = "Graph showing the number of products a customers owns v. churn") +
 theme_minimal() + geom_text(aes(label = ..count..), stat = "count", position = position_dodge(width = 0.8))



## Side-by-side bar graph showing Number of customers having Credit Card v. Churn
card.plot <- ggplot(bank.df) +
 aes(x = HasCrCard, fill = Exited) +
 geom_bar(position = "dodge") +
 scale_fill_brewer(palette = "Accent") +
 labs(x = "Has a Credit Card", title = "Number of customers having Credit Card v. Churn") +
 theme_minimal() + geom_text(aes(label = ..count..), stat = "count", position = position_dodge(width = 0.8))


## Side-by-side bar graph showing Region of the customers v. Churn
region.plot <- ggplot(bank.df) +
 aes(x = Geography, fill = Exited) +
 geom_bar(position = "dodge") +
 scale_fill_brewer(palette = "Accent") +
 labs(x = "Region", y = "count", title = "Region of the customers v. Churn") +
 theme_minimal() + geom_text(aes(label = ..count..), stat = "count", position = position_dodge(width = 0.8))


## Side-by-side bar graph showing  whether customer is active v. Churn
isActive.plot <- ggplot(bank.df) +
 aes(x = IsActiveMember, fill = Exited) +
 geom_bar(position = "dodge") +
 scale_fill_brewer(palette = "Accent") +
 labs(x = "Customer active?", y = "count", title = "Graph of whether customer is active v. Churn") +
 theme_minimal() + geom_text(aes(label = ..count..), stat = "count", position = position_dodge(width = 0.8))


## Box-plots showing distribution of Balance for Customers
balance.plot <- ggplot(bank.df) +
 aes(x = Exited, y = Balance, fill = Exited) +
 geom_boxplot() +
 scale_fill_brewer(palette = "Accent") +
 labs(x = "Churned", y = "Balance", title = "Box-plots showing distribution of Balance", fill = "Churned") +
 theme_minimal()


## Side-by-side box-plot showing distribution of Age for customers
age.plot <- ggplot(bank.df) +
 aes(x = Exited, y = Age, fill = Exited) +
 geom_boxplot() +
 scale_fill_brewer(palette = "Accent") +
 labs(x = "Churned", y = "Age (years)", title = "Side-by-side box-plot showing distribution of Age", fill = "Churned") +
 theme_minimal()


## Side-by-side box-plot showing distribution of Salary of customers
salary.plot<- ggplot(bank.df) +
 aes(x = Exited, y = EstimatedSalary, fill = Exited) +
 geom_boxplot() +
 scale_fill_brewer(palette = "Accent") +
 labs(x = "Churned", y = "Salary", title = "Side-by-side box-plot showing Distribution of Salary", fill = "Churned") +
 theme_minimal()


## Side-by-side box-plot showing distribution of Credit Score of customers
creditScore.plot <- ggplot(bank.df) +
 aes(x = Exited, y = CreditScore, fill = Exited) +
 geom_boxplot() +
 scale_fill_brewer(palette = "Accent") +
 labs(x = "Churned", y = "Credit Score", title = "Side-by-side box-plot showing Distribution of Credit Score of customers", fill = "Churned") +
 theme_minimal()


## Correlation Matrix
#Getting only the numerical features
numerical.features <- names(which(sapply(bank.raw.df, is.numeric)))
cor.mat <- round(cor(bank.raw.df[,numerical.features], use = 'pairwise.complete.obs'),2)
melted.cor.mat <- melt(cor.mat)

correlation.plot <- ggplot(melted.cor.mat, aes(x = X1, y = X2, fill = value)) + 
  geom_tile() + 
  geom_text(aes(x = X1, y = X2, label = value)) + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1)) + coord_fixed()


# Combined Bar plots
bar.plot1 <- ggarrange(gender.plot, numOfProducts.plot, common.legend = TRUE, legend = "right",
                      ncol = 1, nrow = 2)


bar.plot2 <- ggarrange( card.plot, region.plot,isActive.plot, common.legend = TRUE, legend = "right",
                      ncol = 1, nrow = 3)


# Combined Box plots
box.plot1 <- ggarrange(balance.plot, age.plot, 
                       common.legend = TRUE, 
                       legend = "right",
                       ncol = 1, nrow = 2)

box.plot2 <- ggarrange(creditScore.plot, salary.plot, 
                       common.legend = TRUE, 
                       legend = "right",
                       ncol = 1, nrow = 2)

```


# **INTRODUCTION**
**Customer Attrition** is when a customer or a client ends their relationship with a business. It is also known as *customer churn*, *defection* or *turnover*. The total number of customers and revenue lost during a given period is measured using gross attrition. In the banking industry, among commercial banks, one of the main competitions is for customers, especially the high-grade customers. Since, customers are directly related to profits, banks must avoid the loss of customers while acquiring new customers. According to Harvard Business Review, a research done by Bain & Company shows that a business' profits increases by 25% to 95% if they increase the customer retention rates by 5%. Keeping the right customers is very valuable and *customer churn rate* is one of the key metrics in understanding if a business is retaining customers or not.

Churn rate should be looked as an opportunity. By the time an increase in a business' churn rate is observed, it is already six or eight months after the point in time a customer is lost. So, it is very important to predict who is going to leave before it is too late and just accept it. Banks should not just look at churn as a metric. Early predictions of churn can help a business answer questions like "*What are we as a business doing to cause a customer turnover?"*, *"How can we better manage our customer relationships to make sure it does not happen?"* *or "What are our customers doing that is contributing to their leaving?"*. According to Qualtrics Banking Report, credit card companies, insurance agencies, credit unions, and banks can have an attrition rate as high as 20-25%. Another important fact to be taken into consideration is that cost of developing a new customer is 5 to 6 times than retaining an old customer. The need of customer churn management in banks has become inevitable due to the effect of global meltdown on economies. More attention is paid to experience, personalized service, diversity and agility by the customers, which factors into their decision of end their relationship with the business. Â Through this project, I will be identifying the important factors contributing to the customer churn and also build a prediction model to classify whether a customer is going to churn or not.


# **DATA DESCRIPTION**
The bank customer dataset consists of **10,000 instances** and there are **14 features** in it. The credit score ranges from 350 to 850 and the bank has customers from France, Germany and Spain. The age of the customers ranges from 18 to 92 and the minimum salary is 11.58 (which seems to be an outlier). The bank offers the customers around 4 products among which product 1 and product 2 seems to be doing well. Around 29.45% of the customers do not have a credit card whereas 70.55% customers do and 48.49% of the customers seem to be inactive while 51.51% of the customers are active. The data definition is as follows:

- RowNumber: Represents the row number.

- CustomerID: Unique value assigned to a customer

- Surname: Contains the surname of the customer

- CreditScore: This contains the credit score which depicts the customer's credit behavior

- Geography: The region to which the customers belong

- Gender: Contains information on the gender of the customer

- Age: Age of the customer

- Tenure: How long the customer has been with the bank

- Balance: The balance amount available in a customer's account

- NumOfProducts: Number of products the customer has taken from the bank

- HasCrCard: Whether the customer has a credit card or not

- IsActiveMember: Whether the customer is an active member or an inactive member

- EstimatedSalary: Salary earned by the customer

- Exited: Whether the customer has churned or not. (This is the variable of interest)


# **DATA PREPROCESSING**
The dataset was obtained from Kaggle due to absence of open-source confidential data. There were no missing values in the dataset. In the real-world, data will be available in all sorts of forms. Identifying missing values and data would be an important step in order to handle them and build a successful model. To build a model to predict customer churn, columns such as "RowNumber", "CustomerID" and "Surname" were irrelevant, so they were excluded from the dataframe. Features such as "NumOfProducts", "HasCrCard", "IsActiveMember", "Tenure" and "Exited" are integers and were changed into categorical type. For certain algorithms like k-Nearest Neighbors, the dataset was normalized and stored into another dataframe.


# **EXPLORATORY DATA ANALYSIS**
From the exploratory data analysis of the dataset, there are numerous features that stand out in relation to customer churn. The distribution of numeric variables shows age is slightly skewed to the right. The balance appears to be normally distributed expect for the accounts having zero balance.
```{r}
numeric.plot
```

## Class Imbalance
From the bar chart showing the distribution of the response variable 'Exited', we can see that 20.37% o the customers have churned and 79.63% of the customers have not churned. It can be said that almost every 5^th^ customer has churned for this dataset. The dataset is imbalanced. Methods like undersampling and oversampling will need to be implemented to reduce this imbalance.
```{r}
response.plot
```

## Gender and Churn
Around 25.07% of the female customers appear to have churned whereas only 16.45% of the male customers have churned. Churn rate is higher in females compared to males.

## Number of Products owned by customer and Churn
Customers who have purchased more than 2 products from the bank appear to churn.

```{r}
bar.plot1
```

## Credit Card and Churn
20.8% of the customers who do not have a credit card appear to churn and around 20.18% of the customers who have a credit card appear to churn.

## Region and Churn
15.86% customers from France, 39.5% customers from Germany and 16% customers from Spain have exited. So, the churn rate is high is Germany as compared to France and Spain.

## Active Customer and Churn 
26.85% of the inactive customers have left the bank as compared to the 14.2% of the active customers.
Active customers are less likely to churn.

```{r}
bar.plot2
```

## Balance and Churn
From the box plot showing the distribution of balance, it can be seen that customers who churn, appear to have higher balance than the ones who donât. This is could be because other banks might be offering them some kind of premium account for customers with high balance. 

## Age and Churn
From the box plot showing the distribution of age of the customers, older customers appear to churn more than the young customers. One reason for this could be that other banks are offering better savings options for old age groups.

```{r}
box.plot1
```

## Credit Score, Salary and Churn
Credit score and Salary of the customers does not seem to have much effect on a customerâs decision to exit the bank.

```{r}
box.plot2
```

## Correlation Matrix:
We don't want our features to be correlated too strongly. If the pairs are strongly correlated it would make including such pairs redundant, since they influence the result in similar manner. The matrix shows us that the features donât appear to be strongly correlated which is good. Number of products and balance are indirectly correlated. 

```{r}
correlation.plot
```

# **PREDICTIVE MODELING**
```{r}
## Creating a Training and Validation dataset
# Split the data using a stratified sampling approach.
set.seed(123)
training.index <- createDataPartition(bank.df$Exited,p=0.8,list = FALSE)
train.df <- bank.df[training.index, ]
valid.df <- bank.df[-training.index, ]


## Checking distribution of 'Exited' feature in Training and Validation dataset
#cat("Training: ")
#round(prop.table(table(train.df$Exited)),4)
#cat("\nValidation: ")
#round(prop.table(table(valid.df$Exited)),4)


# For Unbalanced Classification problem, 
train.df <- SMOTE(Exited ~ ., data.frame(train.df), perc.over = 100, perc.under = 200)
#round(prop.table(table(dplyr::select(train.df, Exited), exclude = NULL)),4)

# creating a dataframe to store results of each algorithm
results.df <- data.frame(Algorithm = character(),
                         Accuracy = double(),
                         Sensitivity = double(),
                         Specificity = double(), 
                         NIR = double(),
                         AUC = double(), stringsAsFactors=FALSE)
```
I created a training dataset containing a random sample of 80% of the observations and remaining 20% observations in the validation dataset. From EDA, it was clear that the dataset has class imbalance. To handle this, the SMOTE function was used. The function oversamples the rare event to synthetically create additional observations of that event. I have also used a classification threshold of 0.5 for all models.

## Logistic Regression
```{r}
set.seed(123)
## Creating a logistic model on Training dataset
logistic.reg <- glm(Exited ~., data = train.df, family = binomial)


logistic.model = train(Exited ~., data = train.df, method = "glm", family = binomial(link = "logit"))

## Force R not to use Exponential Values
options(scipen=999)


#Confusion Matrix for Train DataSet
logistic.reg.pred.prob.train <- predict(logistic.reg, train.df, type = "response")
logistic.reg.pred.train <- factor(ifelse(logistic.reg.pred.prob.train  >= 0.5, "Churn", "Not Churn"))
train.reco <- factor(ifelse(train.df$Exited  == 1 , "Churn", "Not Churn"))
cM.logistic.train <- confusionMatrix(logistic.reg.pred.train, train.reco)
train.df.numeric <- data.frame("Exited" = as.integer(train.df$Exited))

# Confusion Matrix for Validation Dataset
logistic.reg.pred.prob <- predict(logistic.reg, valid.df, type = "response")
logistic.reg.pred <- factor(ifelse(logistic.reg.pred.prob  >= 0.5, "Churn", "Not Churn"))
valid.reco <- factor(ifelse(valid.df$Exited  == 1 , "Churn", "Not Churn"))
cM.logistic.valid <- confusionMatrix(logistic.reg.pred, valid.reco)

valid.df.numeric <- data.frame("Exited" = as.integer(valid.df$Exited))
#valid.df$Exited <- as.integer(valid.df$Exited)


## ROC
ROC.logistic <- roc(valid.df$Exited,logistic.reg.pred.prob)

# Plotting the ROC curve
logistic.roc.plot <- ggplot() + geom_path(aes(y=ROC.logistic$sensitivities, x=1-ROC.logistic$specificities)) + 
  labs(x = "False Positive Rate", y = "True Positive Rate", title="ROC for Logistic Regression") +  theme_minimal()

# Area under the curve (AUC)
#auc(ROC.logistic)

logistic.nir <- as.numeric(0.7964)

#Updating results.df
results.df <- results.df %>% add_row(Algorithm = "Logistic Regression",
              Accuracy = as.numeric(cM.logistic.valid$overall['Accuracy']),
              Sensitivity = as.numeric(cM.logistic.valid$byClass['Sensitivity']),
              Specificity = as.numeric(cM.logistic.valid$byClass['Specificity']),
              NIR = logistic.nir,
              AUC = as.numeric(auc(ROC.logistic)))


```
Logistic regression is a statistical analysis method used to predict a data value based pn prior observations of a dataset. Logistic Regression model predicts a dependent variable by analyzing the relationship between one or more existing independent variables. For the dataset, using the glm() function, I calculated the coefficient for every attribute. From the summary, it can be seen that the significant features are: Geography (Germany compared to France), Gender (Male compared to Female), Age, NumOfProducts (2 or 3 products compared to 1) and IsActiveMember (1 - yes compared to 0 - no).
```{r}
## Viewing the summary of the logistic model
summary(logistic.reg)

##Interpreting the model coefficients pf the logistic model
cat("\nInterpreting Coefficients: \n")
logistic.reg.coeff <- round(exp(coef(logistic.reg)),2)
logistic.reg.coeff
```
To interpret the significant model coefficients, it can be said that, Given the customer is from Germany, the log odds of churning (versus not churning) increases by 0.8483 or it increases by a factor of 2.34 or 134% as compared to customers from France and Spain. So, German customers are more likely to churn. If the customer is male, the log odds of churning changes by -0.424 or the chances of churning decreases by a factor of 0.65 or 35% compared to female customers. If the customer is associated with two products from the bank, the risk of the customer churning decreases by a factor of 0.36 or 64%, that is, customers who have two products are 64% likely to not churn. The log of odds of churning changes by -1.029. Likewise, given the customer associated with 3 products from the bank, the risk of the customer getting churned is increased by a factor of 22.24, that is, they are extremely likely to churn. The log of odds of churning changes by 3.10. If the customer is an active member of the bank, the risk of the customer getting churned decreased by a factor of 0.50 or 50%. The log of odds of churning changes by -0.701.
```{r}
# Logsitic Regression: Confusion matrix for Validation Dataset
cat("Confusion Matrix for Training Dataset: ")
cM.logistic.train
# Logsitic Regression: Confusion matrix for Validation Dataset
cat("Confusion Matrix for Validation Dataset: ")
cM.logistic.valid
```
For training dataset, I obtained an accuracy of 75.58% and for validation dataset, the accuracy is 77.14%. The AUC for the logistic regression model was found to be 0.8325

Gain Chart:
```{r}

## Gain Chart for Logistic Model
gain <- gains(valid.df.numeric$Exited, logistic.reg.pred.prob, groups=10)
# plot lift chart
plot(c(0,gain$cume.pct.of.total*sum(valid.df.numeric$Exited))~c(0,gain$cume.obs), 
     xlab="# cases", ylab="Cumulative", main="", type="l")
lines(c(0,sum(valid.df.numeric$Exited))~c(0, dim(valid.df.numeric)[1]), lty=2)

```

```{r}
logistic.roc.plot
auc(ROC.logistic)
```


## Decision Trees
```{r}
set.seed(123)
# Using cp = 0.003, maxdepth = 5 and minsplit as 1 for best results
decision.tree <- rpart(Exited ~ ., data = train.df, method = "class", cp = 0.003, maxdepth = 5, minsplit = 1)

# printing tree summary and plot tree. 
#printcp(decision.tree)

#prp(decision.tree, type = 1, extra = 2, under = TRUE, split.font = 1, varlen = -10)

# Generating confusion matrix for training data
decision.tree.pred.train <- predict(decision.tree, train.df, type = "class")
cM.decisionTree.train <- confusionMatrix(decision.tree.pred.train, as.factor(train.df$Exited), positive = "1")



# Generating confusion matrix for Validation data
decision.tree.pred.valid <- predict(decision.tree, valid.df, type = "prob")[,2]
dT.reg.pred <- factor(ifelse(decision.tree.pred.valid  >= 0.5, "Churn", "Not Churn"))
dT.valid.reco <- factor(ifelse(valid.df$Exited  == 1 , "Churn", "Not Churn"))
cM.decisionTree.valid <- confusionMatrix(dT.reg.pred, dT.valid.reco)

# Decision Tree using ctree function
# c.tree <- ctree(Exited ~ ., train.df)
# treePredTrain <- predict(c.tree, train.df, type = "response")
# table(treePredTrain)
# table(train.df$Exited)
# confusionMatrix(treePredTrain, as.factor(train.df$Exited), positive = "1")

#PRINT RULES
#rpart.rules(decision.tree, style="tall", nn = TRUE)

# ROC of Decision Tree
ROC.decisionTree <- roc(valid.df$Exited,(decision.tree.pred.valid))

# Area under the curve (AUC)
#auc(ROC.decisionTree)


dT.nir <- as.numeric(0.7964)

results.df <- results.df %>% add_row(Algorithm = "Decision Tree",
              Accuracy = as.numeric(cM.decisionTree.valid$overall['Accuracy']),
              Sensitivity = as.numeric(cM.decisionTree.valid$byClass['Sensitivity']),
              Specificity = as.numeric(cM.decisionTree.valid$byClass['Specificity']),
              NIR = dT.nir,
              AUC = as.numeric(auc(ROC.decisionTree)))
```
Decision Trees are useful supervised machine learning algorithms which can perform both regression and classification tasks. Decision trees are characterized by nodes and branches where the tests on each attribute are represented at the nodes and the outcome of this procedure is represented at the branches while the class labels are represented at the leaf nodes. Using complexity parameter as 0.003 with maximum depth of 5 and minimum split of 1, the following decision tree was generated.
```{r}
# Plotting decision tree
fancyRpartPlot(decision.tree, sub="Churn Data for Bank Customers")
```
From the decision tree, it can be inferred that there are chances of customers churning if their age is >= 40 and they own three or more products. From Node 13, it can be said that if the customerâs age is between 37 and 40, and they are from Germany who have only one product, then their chances of churning are high.
```{r}
# Decision Tree: Confusion matrix for Training Dataset
cat("Confusion Matrix for Training Dataset: ")
cM.decisionTree.train
# Decision Tree: Confusion matrix for Validation Dataset
cat("Confusion Matrix for Validation Dataset: ")
cM.decisionTree.valid
```
For training dataset, I obtained an accuracy of 78.07% and for validation dataset, the accuracy is 77.69%. The AUC for the logistic regression model was found to be 0.7979

```{r}
# Plotting the ROC curve
dT.roc.plot <- ggplot() + geom_path(aes(y=ROC.decisionTree$sensitivities, x=1-ROC.decisionTree$specificities)) + 
  labs(x = "False Positive Rate", y = "True Positive Rate", title="ROC for Decision Tree") +  theme_minimal()
dT.roc.plot
auc(ROC.decisionTree)
```


## Random Forest
```{r}
set.seed(123)

# 
# ctrl <- trainControl(method = "cv",
#                      number = 10,
#                      selectionFunction = "best")
# 
# ## Create a grid search based on the available parameters.
# grid <- expand.grid(.mtry = c(1:8))
# 
# ## Build the random forest model
# rf.mod <-
#   train(Exited ~.,
#         data = train.df,
#         method = 'rf',
#         metric = 'Kappa',
#         trControl = ctrl,
#         tuneGrid = grid)
# 
# rf.mod

#Kappa was used to select the optimal model using the largest value.
#The final value used for the model was mtry = 8.
random.Forest = randomForest(Exited ~., data=train.df, mtry=8, importance=TRUE)

# Confusion Matrix for Training Data 
randomForest.pred.train <- predict(random.Forest, train.df, type="prob")[,2]
rF.reg.pred.train <- factor(ifelse(randomForest.pred.train  >= 0.5, "Churn", "Not Churn"))
rF.train.reco <- factor(ifelse(train.df$Exited  == 1 , "Churn", "Not Churn"))
cM.randomForest.train <- confusionMatrix(rF.reg.pred.train, rF.train.reco)

# Confusion Matrix for Validation Data 
randomForest.pred <- predict(random.Forest, valid.df, type="prob")[,2]
rF.reg.pred <- factor(ifelse(randomForest.pred  >= 0.5, "Churn", "Not Churn"))
rF.valid.reco <- factor(ifelse(valid.df$Exited  == 1 , "Churn", "Not Churn"))
cM.randomForest.valid <- confusionMatrix(rF.reg.pred, rF.valid.reco)



# ROC of Random Forest
ROC.randomForest <- roc(valid.df$Exited,(randomForest.pred))


rF.nir <- as.numeric(0.7964)

#Updating results.df
results.df <- results.df %>% add_row(Algorithm = "Random Forest",
              Accuracy = as.numeric(cM.randomForest.valid$overall['Accuracy']),
              Sensitivity = as.numeric(cM.randomForest.valid$byClass['Sensitivity']),
              Specificity = as.numeric(cM.randomForest.valid$byClass['Specificity']),
              NIR = rF.nir,
              AUC = as.numeric(auc(ROC.randomForest)))
```
Random Forest is a very powerful ensemble machine learning algorithm which works by creating multiple decision trees and then combines the output generated by each of the decision trees. To select an optimal model kappa was used with the largest value using the grid search definition. So, the final value for the model was mtry = 8. The Out-of-bag error estimate that is the cases which were not used while building the tree is 15.81% for the random forest model.
```{r}
random.Forest
importance(random.Forest, type =1)
```
From the random forest model, we can infer that the important factors in deciding whether a customer churns or not are Age, NumOfProducts, Balance, Tenure, Geography and IsActiveMember.

```{r}
# Random Forest: Confusion matrix for Validation Dataset
cat("Confusion Matrix for Validation Dataset: ")
cM.randomForest.valid
```
The accuracy of the random forest model was found to be 76.54% for the validation dataset. The accuracy is little lower as compared to the accuracy obtained from Decision Tree (77.69%). But, there is a great improvement in the AUC as the AUC for Random Forest Model is 0.838 as compared to only 0.7979 of the Decision Tree. The recall or the sensitivity for the Random forest model (0.751) is also better than Decision tree (0.673).
```{r}
# Plotting the ROC curve for Random Forest
rF.roc.plot <- ggplot() + geom_path(aes(y=ROC.randomForest$sensitivities, x=1-ROC.randomForest$specificities)) + 
  labs(x = "False Positive Rate", y = "True Positive Rate", title="ROC for Random Forest") +  theme_minimal()
rF.roc.plot

# Area under the curve (AUC) for Random Forest
auc(ROC.randomForest)
```


## Support Vector Machines
```{r}

# Tune function to get the best cost
# svm.tune <- tune(svm, 
#                  Exited~. ,
#                  data = train.df,
#                  kernel = "radial",
#                  ranges = list(cost = seq(1, 10, by = 1)))
# summary.svm.tune <- summary(svm.tune.lin)
# summary.svm.tune
# best.svm <- svm.tune.lin$best.model$cost

support.vector <- svm(Exited~. , 
                   data = train.df, 
                   kernel = "radial",
                   cost = 20,
                   probability = TRUE)



# Confusion Matrix for Training Data 
svm.pred.train <- predict(support.vector, train.df, probability = TRUE)
cM.supportVector.train <- confusionMatrix(svm.pred.train, train.df$Exited, positive = "1")


# Confusion Matrix for Validation Data 
svm.pred.valid <- predict(support.vector, valid.df, probability = TRUE)
cM.supportVector.valid <- confusionMatrix(svm.pred.valid, valid.df$Exited, positive = "1")



svm.pred.valid.df<- data.frame(attr(svm.pred.valid, "probabilities"))

ROC.svm <- roc(response = valid.df$Exited, predictor = (svm.pred.valid.df$X1))


svm.nir <- as.numeric(0.7964)

#Updating results.df
results.df <- results.df %>% add_row(Algorithm = "Support Vector Machine",
              Accuracy = as.numeric(cM.supportVector.valid$overall['Accuracy']),
              Sensitivity = as.numeric(cM.supportVector.valid$byClass['Sensitivity']),
              Specificity = as.numeric(cM.supportVector.valid$byClass['Specificity']),
              NIR = svm.nir,
              AUC = as.numeric(auc(ROC.svm)))
```
Support Vector Machine is a supervised learning model in which classification is performed by finding the hyperplane that best differentiates two classes. The objective is to find a hyperplane that that the maximum margin or the maximum distance between the data points of classes. SVMs can take any shape like linear, radial, polynomial, among others and are flexible enough to be used in almost any classification endeavor. For our model, I have created a support vector machine with radial kernel with a cost of 20.

```{r}
# Printing the summary of the Support Vectors
summary(support.vector)
```
The number of support vectors created were 3262 out of which 1660 belonged to level 0 (Not Churned) and 1602 belonged to level 1 (churned).
```{r}
# SVM: Confusion matrix for Training Dataset
cat("Confusion Matrix for Training Dataset: ")
cM.supportVector.train
# SVM: Confusion matrix for Validation Dataset
cat("Confusion Matrix for Validation Dataset: ")
cM.supportVector.valid
```
Accuracy of 82.78% was obtained for the training dataset. We can classify 297 out of 407 "Churned" cases correctly and 1260 out of 1592 "Not Churned" cases correctly. This means the ability of SVM to predict "Churned" cases is about 72.97%Â  and "Not Churned" cases is about 79.15% resulting in overall accuracy of 77.89%. The AUC for SVM is 0.8395.
```{r}
# Plotting ROC for SVM 
svm.roc.plot <- ggplot() + geom_path(aes(y=ROC.svm$sensitivities, x=1-ROC.svm$specificities)) + 
  labs(x = "False Positive Rate", y = "True Positive Rate", title="ROC for SVM") +  theme_minimal()
svm.roc.plot
# Area under the curve (AUC) for SVM
auc(ROC.svm)
```



## eXtreme Gradient Boosting (XGB)
```{r}
## Create a control object
ctrl <-
  trainControl(method = "cv",
               number = 10,
               selectionFunction = "best")
# 
# # modelLookup("xgbTree")
# #
# grid <- expand.grid(
#    nrounds = 80,
#    max_depth = 4,
#    eta =  c(0.1,0.2,0.3,0.4,0.5),
#    gamma = 0.01,
#    colsample_bytree = 1,
#    min_child_weight = 1,
#    subsample = c(0.5, 1)
#  )
# #
# # ## Build XGBoost
#  set.seed(1234)
#  xgb.mod <-
#    train(
#      Exited ~ .,
#      data = train.df,
#      method = "xgbTree",
#      metric = "Kappa",
#      trControl = ctrl,
#      tuneGrid = grid
#    )
# #
#  xgb.mod

best.grid <- expand.grid(max_depth = 6,
                         eta =  0.5,
                         nrounds =1,
                         gamma = 0.01,
                         colsample_bytree = 1,
                         min_child_weight = 1,
                         subsample = 1)

## Build XGBoost
set.seed(123)
best.xgb.mod <- train(Exited ~ .,
                      data = train.df,
                      method = "xgbTree",
                      metric = "Kappa",
                      trControl = ctrl,
                      tuneGrid = best.grid)


# Confusion Matrix for Train Data 
xgb.pred.train <- predict(best.xgb.mod, train.df, type="prob")[,2]
xgb.reg.pred.train <- factor(ifelse(xgb.pred.train  >= 0.5, "Churn", "Not Churn"))
xgb.train.reco <- factor(ifelse(train.df$Exited  == 1 , "Churn", "Not Churn"))
cM.xGB.train <- confusionMatrix(xgb.reg.pred.train, xgb.train.reco)



# Confusion Matrix for Validation Data 
xgb.pred.valid <- predict(best.xgb.mod, valid.df, type="prob")[,2]
xgb.reg.pred.valid <- factor(ifelse(xgb.pred.valid  >= 0.5, "Churn", "Not Churn"))
xgb.valid.reco <- factor(ifelse(valid.df$Exited  == 1 , "Churn", "Not Churn"))
cM.xGB.valid <- confusionMatrix(xgb.reg.pred.valid, xgb.valid.reco)



# ROC of XGB
ROC.xGB <- roc(valid.df$Exited,(xgb.pred.valid))



xgb.nir <- as.numeric(0.7964)
#Updating results.df
results.df <- results.df %>% add_row(Algorithm = "eXtreme Gradient Boosting",
              Accuracy = as.numeric(cM.xGB.valid$overall['Accuracy']),
              Sensitivity = as.numeric(cM.xGB.valid$byClass['Sensitivity']),
              Specificity = as.numeric(cM.xGB.valid$byClass['Specificity']),
              NIR = xgb.nir,
              AUC = as.numeric(auc(ROC.xGB)))
```
XGBoost (eXtreme Gradient Boosting) algorithm is used for supervised learning tasks. It is similar to gradient boosting framework but more efficient. It belongs to a family of boosting algorithms that convert weak learners to strong learners. Trees are grown using the information from a previously grown tree one after the other, so boosting is a sequential process. This improves predictions in subsequent iterations. Misclassification rate is also reduced in subsequent iterations. XGBoost works only with numeric vectors. The maximum depth was chosen as 6 (the tree won't be deep as our case is simple), gamma was 0.01 and the learning parameter 'eta' was chosen as 0.5 after tuning the model. The learning parameter controls how much information from a new tree will be used in the Boosting whereas the gamma parameter controls the minimum reduction required to grow a new node in a tree.

```{r}
best.xgb.mod
```

```{r}
# xGB: Confusion matrix for Training Dataset
cat("Confusion Matrix for Training Dataset: ")
cM.xGB.train
# xGB: Confusion matrix for Validation Dataset
cat("Confusion Matrix for Validation Dataset: ")
cM.xGB.valid
```
The accuracy obtained from the model is the highest at 79.93% for the validation dataset and it is 79.85 for the training dataset. We can classify 257 out of 407 "Churned" cases correctly and 1341 out of 1592 "Not Churned" cases correctly. This means the ability of XGB to predict "Churned" cases is about 63.14%Â  and "Not Churned" cases is about 84.3% resulting in overall accuracy of 79.94%. The AUC for XGB model is 0.8249.
```{r}
# Plotting the ROC curve
xgb.roc.plot <- ggplot() + geom_path(aes(y=ROC.xGB$sensitivities, x=1-ROC.xGB$specificities)) + 
  labs(x = "False Positive Rate", y = "True Positive Rate", title="ROC for xGB") +  theme_minimal()
xgb.roc.plot
# Area under the curve (AUC)
auc(ROC.xGB)

```


## NaÃ¯ve Bayes
```{r}
# Build Model
naiveBayes.model <-  naiveBayes(Exited ~ ., data = train.df)


# Confusion Matrix for Training Data 
nB.pred.train <- predict(naiveBayes.model, train.df, type="raw")[,2]
nB.reg.pred.train <- factor(ifelse(nB.pred.train  >= 0.5, "Churn", "Not Churn"))
nB.train.reco <- factor(ifelse(train.df$Exited  == 1 , "Churn", "Not Churn"))
cM.nB.train <- confusionMatrix(nB.reg.pred.train, nB.train.reco)


# Confusion Matrix for Validation Data 
nB.pred.valid <- predict(naiveBayes.model, valid.df, type="raw")[,2]
nB.reg.pred.valid <- factor(ifelse(nB.pred.valid  >= 0.5, "Churn", "Not Churn"))
nB.valid.reco <- factor(ifelse(valid.df$Exited  == 1 , "Churn", "Not Churn"))
cM.nB.valid <- confusionMatrix(nB.reg.pred.valid, nB.valid.reco)



# ROC of Random Forest
ROC.nB <- roc(valid.df$Exited,(nB.pred.valid))


nB.nir <- as.numeric(0.7964)
#Updating results.df
results.df <- results.df %>% add_row(Algorithm = "Naive Bayes",
              Accuracy = as.numeric(cM.nB.valid$overall['Accuracy']),
              Sensitivity = as.numeric(cM.nB.valid$byClass['Sensitivity']),
              Specificity = as.numeric(cM.nB.valid$byClass['Specificity']),
              NIR = nB.nir,
              AUC = as.numeric(auc(ROC.nB)))

```
NaÃ¯ve Bayes is a supervised machine learning algorithm that uses a probabilistic approach. Since the predictors aren't always independent of each other, there are always some correlations between them. NaÃ¯ve Bayes considers each predictor to be independent of each other. NaÃ¯ve Bayes classifiers can perform well even with high-dimensional data points or large number of data points. The model creates conditional probability for each feature separately and we also get a-priori probabilities which indicate the distribution of the data. The a-priori probabilities and the conditional probabilities of Churn in the Bank dataset is as follows:
```{r}
naiveBayes.model
```
From the result, we can infer that, given the customer is from Germany, there are 38.15% chances of them churning or given the customer is a female, there are 52.68% chances of churning. Similarly, attrition can be deduced for the other features using the conditional probabilities of the NaÃ¯ve Bayes Model.
```{r}
# nB: Confusion matrix for Training Dataset
cat("Confusion Matrix for Training Dataset: ")
cM.nB.train
# nB: Confusion matrix for Validation Dataset
cat("Confusion Matrix for Validation Dataset: ")
cM.nB.valid
```
We can classify 288 out of 407 "Churned" cases correctly and 1214 out of 1592 "Not Churned" cases correctly. This means the ability of NaÃ¯ve Bayes to predict "Churned" cases is about 70.76% and "Not Churned" cases is about 76.26% resulting in overall accuracy of 75.14%. The AUC for NaÃ¯ve Bayes model is 0.8214.
```{r}

# Plotting the ROC curve
nB.roc.plot <- ggplot() + geom_path(aes(y=ROC.nB$sensitivities, x=1-ROC.nB$specificities)) + 
  labs(x = "False Positive Rate", y = "True Positive Rate", title="ROC for xGB") +  theme_minimal()
nB.roc.plot
# Area under the curve (AUC)
auc(ROC.nB)
```


## K-Nearest Neighbors
```{r}

# Converting training dataset predictors to numeric data type for KNN
nn.train.df <- train.df
nn.train.df$Geography <- as.numeric(nn.train.df$Geography)
nn.train.df$Gender <- as.numeric(nn.train.df$Gender)
nn.train.df$Tenure <- as.numeric(nn.train.df$Tenure)
nn.train.df$NumOfProducts <- as.numeric(nn.train.df$NumOfProducts)
nn.train.df$HasCrCard <- as.numeric(nn.train.df$HasCrCard)
nn.train.df$IsActiveMember <- as.numeric(nn.train.df$IsActiveMember)
nn.train.df$Exited <- as.numeric(nn.train.df$Exited)


# Converting validation dataset predictors to numeric data type for KNN
nn.valid.df <- valid.df
nn.valid.df$Geography <- as.numeric(nn.valid.df$Geography)
nn.valid.df$Gender <- as.numeric(nn.valid.df$Gender)
nn.valid.df$Tenure <- as.numeric(nn.valid.df$Tenure)
nn.valid.df$NumOfProducts <- as.numeric(nn.valid.df$NumOfProducts)
nn.valid.df$HasCrCard <- as.numeric(nn.valid.df$HasCrCard)
nn.valid.df$IsActiveMember <- as.numeric(nn.valid.df$IsActiveMember)
nn.valid.df$Exited <- as.numeric(nn.valid.df$Exited)


# Scaling training dataset for neural nets
train.scaled.df <- data.frame(scale(nn.train.df[,1:10]))

# Scaling validation dataset for neural nets
valid.scaled.df <- data.frame(scale(nn.valid.df[,1:10]))


# Adding Response variable to the scaled datasets
train.scaled.df <- cbind(train.scaled.df,Exited = train.df$Exited)
valid.scaled.df <- cbind(valid.scaled.df,Exited = valid.df$Exited)

#Create a KNN Model
set.seed(123)
kNN.model <- train(Exited ~ ., data = train.scaled.df, method = "knn", preProcess = c("center","scale"), tuneLength = 7)



# Confusion Matrix for Training Data 
kNN.pred.train <- predict(kNN.model,train.scaled.df, type = "prob")[,2]
kNN.reg.pred.train <- factor(ifelse(kNN.pred.train  >= 0.5, "Churn", "Not Churn"))
kNN.train.reco <- factor(ifelse(train.scaled.df$Exited  == 1 , "Churn", "Not Churn"))
cM.kNN.train <- confusionMatrix(kNN.reg.pred.train, kNN.train.reco)



# Confusion Matrix for Validation Data 
kNN.pred.valid <- predict(kNN.model,valid.scaled.df, type = "prob")[,2]
kNN.reg.pred.valid <- factor(ifelse(kNN.pred.valid  >= 0.5, "Churn", "Not Churn"))
kNN.valid.reco <- factor(ifelse(valid.scaled.df$Exited  == 1 , "Churn", "Not Churn"))
cM.kNN.valid <- confusionMatrix(kNN.reg.pred.valid, kNN.valid.reco)



# ROC of Random Forest
ROC.kNN <- roc(valid.scaled.df$Exited,as.numeric(kNN.pred.valid))


knn.nir <- as.numeric(0.7964)

#Updating results.df
results.df <- results.df %>% add_row(Algorithm = "k-Nearest Neighbor",
              Accuracy = as.numeric(cM.kNN.valid$overall['Accuracy']),
              Sensitivity = as.numeric(cM.kNN.valid$byClass['Sensitivity']),
              Specificity = as.numeric(cM.kNN.valid$byClass['Specificity']),
              NIR = knn.nir,
              AUC = as.numeric(auc(ROC.kNN)))
```
K-Nearest neighbor (KNN) classifies a new data point into the target class, depending of the features of its neighboring data points. It is one of the most simple machine learning algorithms. Determining the value of k, that is, choosing the number of nearest neighbors plays a significant role in determining the efficacy of the model. It determines how well the data can be utilized to generalize the results of the kNN algorithm. A large k value reduces the variance due to the noisy data but it maby also develop a bias. So, it is better to choose an optimal value of k. Based on the accuracy of the optimal model using the largest value, the value of the number of nearest neighbors was chosen to be 17.
```{r}
kNN.model
```

```{r}
# kNN: Confusion matrix for Training Dataset
cat("Confusion Matrix for Training Dataset: ")
cM.kNN.train
# kNN: Confusion matrix for Validation Dataset
cat("Confusion Matrix for Validation Dataset: ")
cM.kNN.valid
```
KNN classified 313 out of 407 "Churned" cases correctly and 1121 out of 1592 "Not Churned" cases correctly. So, the ability of k-Nearest Neighbors to predict "Churned" cases is about 76.9% and "Not Churned" cases are about 70.41% resulting in overall accuracy of 71.75%. The AUC for k-Nearest Neighbors model is 0.8176

```{r}
# Plotting the ROC curve
knn.roc.plot <- ggplot() + geom_path(aes(y=ROC.kNN$sensitivities, x=1-ROC.kNN$specificities)) + 
  labs(x = "False Positive Rate", y = "True Positive Rate", title="ROC for xGB") +  theme_minimal()
knn.roc.plot
# Area under the curve (AUC)
auc(ROC.kNN)
```


# **IMPORTANCE OF PREDICTORS**
```{r}
# Plotting important predictors for Logistic Model
logistic.important.predictors <- varImp(logistic.model, scale=FALSE)
plot(logistic.important.predictors, main = "Relative importance of predictors in Logistic Regression Model")

# Variable Importance in Random Foreset
vip(random.Forest, scale = T,  aesthetics = list(fill = "#57F2AF")) + labs(y = "Relative importance", title = "Relative importance of predictors in Random Forest model")+
  theme_minimal()

# Importance of Variables
vip(best.xgb.mod, scale = T,  aesthetics = list(fill = "#57F2AF")) + labs(y = "Relative importance", title = "Relative importance of predictors in eXtreme Gradient Boosting")+
  theme_minimal()
```
As it can be seen from the plots for important predictors in Logistic Regression model, Random Forest model and xGB model, the top 6 most important variables (age, numOfPoducts2, Balance, Tenure, numOfProducts3, isActiveMember1, GeographyGermany) for each of the model are close to each other. However, for Random Forest model, Balance and Tenure appear to be quite important compared to Logistic Regression model and xGB model. As for the rest of the predictors, **age**, **numOfPoducts2**, **numOfProducts3**, **isActiveMember1**, **GeographyGermany** constitute a set of variables of high importance. In other words, it can be said that, these predictors are very useful in predicting customer's churning behavior for this given dataset.

# **COMPARISON OF MODELS**
```{r}
algorithm.colors <- c("k-Nearest Neighbor" = "brown3", "Logistic Regression" = "cadetblue2", "Decision Tree" = "darkorchid4", "Random Forest" = "palevioletred1", "Support Vector Machine" = "pink4", "eXtreme Gradient Boosting" = "seagreen","Naive Bayes" = "goldenrod1")

ggplot() + 
  geom_path(aes(y=ROC.kNN$sensitivities, x=1-ROC.kNN$specificities,  color = "k-Nearest Neighbor"), size = 1.2) + 
  geom_path(aes(y=ROC.nB$sensitivities, x=1-ROC.nB$specificities, color = "Naive Bayes"), size = 1.2) +
  geom_path(aes(y=ROC.xGB$sensitivities, x=1-ROC.xGB$specificities, color = "eXtreme Gradient Boosting"), size = 1.2) + 
  geom_path(aes(y=ROC.svm$sensitivities, x=1-ROC.svm$specificities, color = "Support Vector Machine"), size = 1.2) + 
  geom_path(aes(y=ROC.randomForest$sensitivities, x=1-ROC.randomForest$specificities, color = "Random Forest"), size = 1.2) + 
  geom_path(aes(y=ROC.decisionTree$sensitivities, x=1-ROC.decisionTree$specificities, color = "Decision Tree"), size = 1.2) + 
  geom_path(aes(y=ROC.logistic$sensitivities, x=1-ROC.logistic$specificities, color = "Logistic Regression"), size = 1.2) + 
  labs(x = "False Positive Rate", 
       y = "True Positive Rate", 
       title="Comparison of ROC of various algorithms", 
       color = "Algorithms:") +  
  scale_color_manual(values = algorithm.colors) +
  theme_minimal()
```

```{r}
results.df
```
Comparing all the models with threshold of 0.5, we see that NaÃ¯ve Bayes model has the lowest accuracy of 75.13% as compared to eXtreme Gradient Boosting model with the accuracy of 79.93%. The model with the highest recall or sensitivity, that is the proportion of actual "Churned" that got predicted as "Churned', is Random Forest model with a recall of 75.18%. Support Vector Machine model has the highest AUC of 0.839 among all the models that were created. Considering a **threshold of 0.5**, the No Information Rate is 0.7964. All the other models except the eXtreme Gradient Boosting model have an accuracy lower than the No Information Rate, so they are downright lousy. Now, if XGB model is chosen, it has very low sensitivity. Considering all the aspects, I would go with Random Forest model as it has a good accuracy (76.53%), good recall (75.18%), better specificity (76.88%) and has the second highest AUC (0.837).

# **CONCLUSION**
It is important to identify the attrition rate for a business, as it tells how the business is doing. If the attrition rate is high, it indicates that there is some problem with the organization. Once the churning customer clusters are predicted, banks can take decisions to provide personalized offers to retain customers and save their revenue. From the variable importance charts, we can see that age has the highest impact on customer attrition. Higher the age of the customer, more likely they are to churn. Banks should come up with premium accounts and offer preference to such customers' needs. Another issue could be the fees associated with the maintenance of these accounts. These customers should feel that they are being taken care of by offering them better customer experience. We also see that the number of products used by a customer has a very huge impact on customer churn. People who use more than two products are very likely to churn. This could be due to the fees associated with the products of the return rates offered for these products are less as compared to the competitors. Banks could implement long term strategy to offer lower fees on entry-level products and better rates on more profitable products, this will help to increase the customer lifetime value and increase the loyalty. The rate of customer churn in the customers from Germany is also very high. One reason behind this could be limited number of physical branches of the bank in the area or lack of in-person guidance from a valued advisors. Banks should start examining why this could happen and the marketing team in Germany should spend more time in retaining customers in the region.

With a better dataset, the customers prone to churning can be predicted using the classification algorithms. As not everyone can be pleased, so customer attrition is an unavoidable problem, but it can be addressed as early as possible to understand what processes or factors can be improved and looked after.


# **REFERENCES**
1. <https://www.kaggle.com/mathchi/churn-for-bank-customers>

2. <https://hbr.org/2014/10/the-value-of-keeping-the-right-customers>

3. <https://www.qualtrics.com/blog/customer-churn-banking/>

4. <https://www.hranalytics101.com/how-to-assess-model-accuracy-the-basics/>
